{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Tokenization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUIheBWAEr61"
      },
      "source": [
        "# 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxCOcTPYNgpD"
      },
      "source": [
        "## 1. Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICh35St_-9Pz"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2JGNtVzCdlM",
        "outputId": "db121d84-9e1c-4931-ddd5-e7bdef1ed468"
      },
      "source": [
        "samples = ['The cat sits on the mat','The other cat % $ # @ runs over the mats']\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 12)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "tokenizer"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7ffaa91b1650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPxDXXAvGEhW",
        "outputId": "23afe84c-1802-4f48-9598-1b3048b75a89"
      },
      "source": [
        "# 단어와 각 단어에 부여된 index를 dictionary 형태로 출력\n",
        "word_idx = tokenizer.word_index\n",
        "print(word_idx)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'cat': 2, 'sits': 3, 'on': 4, 'mat': 5, 'other': 6, 'runs': 7, 'over': 8, 'mats': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElOHOcpXGSeu",
        "outputId": "242111bb-d5a9-44c3-8e17-3bdaf5cf3399"
      },
      "source": [
        "# 단어로 토큰화한 텍스트를 word_idx의 인덱스로 바꿈\n",
        "seq = tokenizer.texts_to_sequences(samples)\n",
        "print(seq)\n",
        "print('\\n')\n",
        "\n",
        "# sequence 길이 맞추기 위해 zero-padding\n",
        "pad_seq = sequence.pad_sequences(seq, maxlen = 7)\n",
        "print(pad_seq)\n",
        "print('\\n')\n",
        "\n",
        "# sequence를 binary(단어 있으면 1, 아니면 0)으로 바꿈\n",
        "mat1 = tokenizer.sequences_to_matrix(seq, mode = 'binary')\n",
        "print(mat1)\n",
        "print('\\n')\n",
        "\n",
        "# sequence를 해당 단어의 빈도수로 바꿈\n",
        "mat2 = tokenizer.sequences_to_matrix(seq, mode = 'count')\n",
        "print(mat2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 3, 4, 1, 5], [1, 6, 2, 7, 8, 1, 9]]\n",
            "\n",
            "\n",
            "[[0 1 2 3 4 1 5]\n",
            " [1 6 2 7 8 1 9]]\n",
            "\n",
            "\n",
            "[[0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.]]\n",
            "\n",
            "\n",
            "[[0. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 2. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gOiKkYsNkbO"
      },
      "source": [
        "## 2. NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9EaKkC2Nj-Z",
        "outputId": "f9e467fa-463b-437e-a643-013c31f053b3"
      },
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE8xfM-MOJlA",
        "outputId": "8a492f3f-56c2-4abc-bdd7-cbbe083cdc6d"
      },
      "source": [
        "# 문장단위 토큰화\n",
        "sample_sent = 'I love dogs. I also like cats. I really like animals'\n",
        "sentences = sent_tokenize(text = sample_sent)\n",
        "print(sentences)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I love dogs.', 'I also like cats.', 'I really like animals']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGNs1sSrPDMR",
        "outputId": "c69b5221-e202-46e2-bf2f-270f183f9908"
      },
      "source": [
        "# 단어단위 토큰화\n",
        "sample_sent = 'I have been raising a cute maltese dog since 2011.'\n",
        "words = word_tokenize(sample_sent)\n",
        "print(words)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'have', 'been', 'raising', 'a', 'cute', 'maltese', 'dog', 'since', '2011', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wy2A-nVP74V"
      },
      "source": [
        "### 스톱워드(Stop word)\n",
        "정의: 텍스트 분석에 큰 의미가 없는 단어를 말하는 것으로, 영어로 치면 'a, the, is, am, were, been' 등 문법적인 구성요소를 포함한다. 이 단어들은 문장에 많이 포함되어 있기 때문에 제거하지 않으면 이 단어들을 중요한 단어로 인식하여 텍스트분석의 정확도를 떨어뜨릴 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwFExjExP6bu",
        "outputId": "993f857c-4628-4d5e-8153-fe71ab6bef2a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc4nJiZBQe6P",
        "outputId": "e492a170-8861-472a-8c50-e6951dd47be8"
      },
      "source": [
        "# 영어 스톱워드 목록 생성\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "new_words = []\n",
        "for word in words:\n",
        "  word = word.lower()\n",
        "  if word not in stopwords:\n",
        "    new_words.append(word)\n",
        "  \n",
        "print(new_words)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['raising', 'cute', 'maltese', 'dog', 'since', '2011', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
